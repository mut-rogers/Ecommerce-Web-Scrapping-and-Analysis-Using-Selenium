{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver \n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Beautiful Soup \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Automates downloading and setting up of Chrome Driver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Custom Settings for the project \n",
    "import settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page(driver, current_url: str) -> list: \n",
    "    \"\"\"    \n",
    "    This functions scrapes a single webpage in a paginated website. \n",
    "\n",
    "    Args: \n",
    "        driver: a webdriver oject\n",
    "        current_url: the url that will be visited by the driver\n",
    "\n",
    "    Returns: \n",
    "        List of products scraped from a single page/current_url\n",
    "    \"\"\" \n",
    "\n",
    "    driver.get(current_url) # Visit the web page \n",
    "\n",
    "    # wait for the page to fully load\n",
    "    wait = WebDriverWait(driver, 4)\n",
    "    wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\"))) \n",
    "\n",
    "    html_content = driver.page_source # Access the HTML source \n",
    "    \n",
    "    soup = BeautifulSoup(html_content, \"html.parser\") # Parse using Beautiful Soup \n",
    "\n",
    "    products_wrapper = soup.find_all(\"div\", {\"class\": \"listing-item\"}) # Access the products container \n",
    "\n",
    "    all_scrapped_products = list()\n",
    "\n",
    "    # Loop through the products_wrapper to access each product cantainer\n",
    "    # Then extract the prodcut information \n",
    "    for product in products_wrapper:\n",
    "        product_name = product.find(\"p\", {\"class\": \"product-title\"}).text \n",
    "        product_price = product.find(\"div\", {\"class\": \"product-price\"}).text\n",
    "        num_reviews = product.find(\"span\", {\"class\": \"reviews\"}).text\n",
    "        product_source = product.find(\"span\", {\"class\": \"tag-name\"}).text # Where the product was sourced: Locally or Shipped\n",
    "        \n",
    "        # Wrap the produce in a dictionary and append to the list\n",
    "        scrapped_product_dict = {\n",
    "            \"product_name\": product_name,\n",
    "            \"product_price\": product_price,\n",
    "            \"number_reviews\": num_reviews,\n",
    "            \"product_source\": product_source\n",
    "        } \n",
    "        all_scrapped_products.append(scrapped_product_dict) \n",
    "\n",
    "    return all_scrapped_products\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'settings' has no attribute 'PRODUCT_CATEGORIES'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m# driver.quit()\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 40\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBASE_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[39], line 27\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(BASE_URL, num_pages)\u001b[0m\n\u001b[0;32m     24\u001b[0m all_products \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m() \u001b[38;5;66;03m# list of dicts To store the scrapped products\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Scrapping each product category at a time\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m category_id, category_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43msettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPRODUCT_CATEGORIES\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m current_page_nummber \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m num_pages: \u001b[38;5;66;03m# Ensure we don't access pages out of range\u001b[39;00m\n\u001b[0;32m     31\u001b[0m         current_url \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mBASE_URL\u001b[38;5;241m.\u001b[39mformat(category_name, category_id, current_page_nummber) \n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'settings' has no attribute 'PRODUCT_CATEGORIES'"
     ]
    }
   ],
   "source": [
    "\n",
    "def main(BASE_URL: str, num_pages: str): \n",
    "    \"\"\"   \n",
    "    This is the main function that will be used to scrape the entire website. \n",
    "\n",
    "    Args: \n",
    "        BASE_URL: The url of the website to be scrapped. Contains the URL that will dynamically change due to pagination and search parameters such as product category \n",
    "\n",
    "        num_pages: The number of pages to be scrapped on the specified website.\n",
    "     \n",
    "    Returns: \n",
    "        A final list containing a dictionary of products scrapped from the entire website\n",
    "    \"\"\" \n",
    "\n",
    "    # Set up the use of headless Chrome \n",
    "    chrome_browser_options = Options()\n",
    "    chrome_browser_options.headless = True\n",
    "\n",
    "    service = Service(ChromeDriverManager().install()) # Set up Chrome driver\n",
    "\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_browser_options) # Create a webdriver object\n",
    "\n",
    "    current_page_nummber = 1 # Page Counter. Tracks the current web page being scrapped\n",
    "\n",
    "    all_products = list() # list of dicts To store the scrapped products\n",
    "\n",
    "    # Scrapping each product category at a time\n",
    "    for category_id, category_name in settings.PRODUCT_CATEGORIES.items():\n",
    "\n",
    "        while current_page_nummber <= num_pages: # Ensure we don't access pages out of range\n",
    "\n",
    "            current_url = settings.BASE_URL.format(category_name, category_id, current_page_nummber) \n",
    "\n",
    "            current_products_sublist = scrape_page(driver, current_url) \n",
    "            print(\"Scrapping {}\".format(category_id))\n",
    "            current_page_nummber += 1\n",
    "\n",
    "    # driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(settings.BASE_URL, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Downloading and setting up Chrome Driver \n",
    "service = Service(ChromeDriverManager().install()) \n",
    "\n",
    "# initializing the webdriver \n",
    "driver = webdriver.Chrome(service=service) \n",
    "\n",
    "# Navigating to the website \n",
    "driver.get(BASE_URL) \n",
    "\n",
    "# Let the scrapper wait for 5 seconds to ensure page has fully loaded \n",
    "wait = WebDriverWait(driver, 5)\n",
    "wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\"))) \n",
    "\n",
    "# Access the HTML content \n",
    "html_markup = driver.page_source \n",
    "soup = BeautifulSoup(html_markup, \"html.parser\") \n",
    "\n",
    "# Get all the tags containing the products listing \n",
    "products_wrapper = soup.find_all(\"div\", {\"class\": \"listing-item\"})\n",
    "\n",
    "# Loop through the products_wrapper \n",
    "for product in products_wrapper: \n",
    "    product_name = product.find(\"div\", {\"class\": \"product-price\"}).text \n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
