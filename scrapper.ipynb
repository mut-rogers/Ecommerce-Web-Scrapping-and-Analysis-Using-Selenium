{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver \n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Beautiful Soup \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Automates downloading and setting up of Chrome Driver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Custom Settings for the project \n",
    "import settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page(driver, current_url: str, product_category: str) -> list: \n",
    "    \"\"\"    \n",
    "    This functions scrapes a single webpage in a paginated website. \n",
    "\n",
    "    Args: \n",
    "        driver: a webdriver oject\n",
    "        current_url: the url that will be visited by the driver\n",
    "\n",
    "    Returns: \n",
    "        List of products scraped from a single page/current_url\n",
    "    \"\"\" \n",
    "\n",
    "    driver.get(current_url) # Visit the web page \n",
    "\n",
    "    # wait for the page to fully load\n",
    "    wait = WebDriverWait(driver, 4)\n",
    "    wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\"))) \n",
    "\n",
    "    html_content = driver.page_source # Access the HTML source \n",
    "    \n",
    "    soup = BeautifulSoup(html_content, \"html.parser\") # Parse using Beautiful Soup \n",
    "\n",
    "    products_wrapper = soup.find_all(\"div\", {\"class\": \"listing-item\"}) # Access the products container \n",
    "\n",
    "    all_scrapped_products = list()\n",
    "\n",
    "    # Loop through the products_wrapper to access each product cantainer\n",
    "    # Then extract the prodcut information \n",
    "    for product in products_wrapper:\n",
    "        product_name = product.find(\"p\", {\"class\": \"product-title\"}).text \n",
    "        product_price = product.find(\"div\", {\"class\": \"product-price\"}).text\n",
    "        num_reviews = product.find(\"span\", {\"class\": \"reviews\"}).text\n",
    "        product_source = product.find(\"span\", {\"class\": \"tag-name\"}).text # Where the product was sourced: Locally or Shipped\n",
    "        \n",
    "        # Wrap the produce in a dictionary and append to the list\n",
    "        scrapped_product_dict = {\n",
    "            \"product_name\": product_name,\n",
    "            \"product_price\": product_price,\n",
    "            \"number_reviews\": num_reviews,\n",
    "            \"product_category\": product_category,\n",
    "            \"product_source\": product_source,\n",
    "        } \n",
    "        all_scrapped_products.append(scrapped_product_dict) \n",
    "\n",
    "    return all_scrapped_products\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clothes:167:https://www.kilimall.co.ke/category/clothes?id=167&form=category&page=1\n",
      "bags:1:https://www.kilimall.co.ke/category/bags?id=1&form=category&page=1\n"
     ]
    }
   ],
   "source": [
    "PRODUCT_CATEGORIES = {\n",
    "    \"167\": \"clothes\",\n",
    "    \"1\": \"bags\"\n",
    "}\n",
    "\n",
    "# URL for the website.\n",
    "# product category and web page number will change on demand\n",
    "BASE_URL = \"https://www.kilimall.co.ke/category/{}?id={}&form=category&page={}\"\n",
    "\n",
    "def main(BASE_URL: str, num_pages: str): \n",
    "    \"\"\"   \n",
    "    This is the main function that will be used to scrape the entire website. \n",
    "\n",
    "    Args: \n",
    "        BASE_URL: The url of the website to be scrapped. Contains the URL that will dynamically change due to pagination and search parameters such as product category \n",
    "\n",
    "        num_pages: The number of pages to be scrapped on the specified website.\n",
    "     \n",
    "    Returns: \n",
    "        A final list containing a dictionary of products scrapped from the entire website\n",
    "    \"\"\" \n",
    "\n",
    "    # Set up the use of headless Chrome - No UI \n",
    "    chrome_browser_options = Options()\n",
    "    chrome_browser_options.add_argument(\"--headless\")\n",
    "\n",
    "    service = Service(ChromeDriverManager().install()) # Set up Chrome driver\n",
    "\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_browser_options) # Create a webdriver object\n",
    "\n",
    "    current_page_nummber = 1 # Page Counter. Tracks the current web page being scrapped\n",
    "\n",
    "    all_products = list() # list of dicts To store the scrapped products\n",
    "\n",
    "    # Scrapping each product category at a time\n",
    "    for category_id, category_name in PRODUCT_CATEGORIES.items():\n",
    "\n",
    "        while current_page_nummber <= num_pages: # Ensure we don't access pages out of range\n",
    "\n",
    "            current_url = BASE_URL.format(category_name, category_id, current_page_nummber) \n",
    "\n",
    "            current_products_sublist = scrape_page(driver, current_url) \n",
    "            \n",
    "            current_page_nummber += 1\n",
    "\n",
    "        current_page_nummber = 1 # Reset the count after switching to new product categories\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(BASE_URL, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167 clothes\n",
      "1 bags\n"
     ]
    }
   ],
   "source": [
    "PRODUCT_CATEGORIES = {\n",
    "    \"167\": \"clothes\",\n",
    "    \"1\": \"bags\"\n",
    "}\n",
    "\n",
    "for category_id, category_name in PRODUCT_CATEGORIES.items():\n",
    "    print(category_id, category_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Downloading and setting up Chrome Driver \n",
    "service = Service(ChromeDriverManager().install()) \n",
    "\n",
    "# initializing the webdriver \n",
    "driver = webdriver.Chrome(service=service) \n",
    "\n",
    "# Navigating to the website \n",
    "driver.get(BASE_URL) \n",
    "\n",
    "# Let the scrapper wait for 5 seconds to ensure page has fully loaded \n",
    "wait = WebDriverWait(driver, 5)\n",
    "wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\"))) \n",
    "\n",
    "# Access the HTML content \n",
    "html_markup = driver.page_source \n",
    "soup = BeautifulSoup(html_markup, \"html.parser\") \n",
    "\n",
    "# Get all the tags containing the products listing \n",
    "products_wrapper = soup.find_all(\"div\", {\"class\": \"listing-item\"})\n",
    "\n",
    "# Loop through the products_wrapper \n",
    "for product in products_wrapper: \n",
    "    product_name = product.find(\"div\", {\"class\": \"product-price\"}).text \n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
